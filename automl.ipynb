{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from helper import run_inference, get_result_df\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "The Titanic dataset used in this project was downloaded from [Kaggle](https://www.kaggle.com/competitions/titanic/overview). We are provided a train and a test dataset with 891 and 418 records, respectively, with 12 columns.\n",
    "From the given features for each record we want to make a prediction if this person did survive the Titanic disaster or not. Therefore relevant features should be identified to help us with this task.\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workspace from config file present at .\\config.json.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Choose a name for experiment\n",
    "experiment_name = 'Titanic'\n",
    "project_folder = './titanic-project'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitanic_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m description_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the dataset for the capstone project.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mws\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mkeys(): \n\u001b[0;32m      7\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ws\u001b[38;5;241m.\u001b[39mdatasets[key] \n",
      "\u001b[1;31mNameError\u001b[0m: name 'ws' is not defined"
     ]
    }
   ],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "found = False\n",
    "key = \"Titanic_dataset\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        train_data = './data/train_modified.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(train_data, separator=';')        \n",
    "        #Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=\"This is the complete dataset for the capstone project.\")\n",
    "        dataset_filtered = dataset.keep_columns([\"Survived\",\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Age\"])\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key+\"_filtered\",\n",
    "                                   description=\"This is the filtered dataset for the capstone project \" \\\n",
    "                                   \"with only those features relevant for training.\")\n",
    "\n",
    "df = dataset_filtered.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below.\n",
    "\n",
    "First, we create a compute cluster if it is not yet available. As we want to use deep learning, we choose a GPU for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "num_nodes = 5\n",
    "\n",
    "amlcompute_cluster_name = \"ComputeClusterCapstone\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           vm_priority = 'lowpriority',\n",
    "                                                           max_nodes=num_nodes)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598429217746
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"enable_dnn\": True,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"validation_size\": 0.2,\n",
    "    \"primary_metric\" : 'accuracy',\n",
    "    \"enable_voting_ensemble\": False,\n",
    "    \"enable_stack_ensemble\": False\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=dataset_filtered,\n",
    "                             label_column_name=\"Survived\",   \n",
    "                             path = project_folder,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run = remote_run.get_best_child()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "[Source](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "\n",
    "# Download the featurization summary JSON file locally\n",
    "best_run.download_file(\n",
    "    \"outputs/featurization_summary.json\", \"featurization_summary.json\"\n",
    ")\n",
    "\n",
    "# Render the JSON as a pandas DataFrame\n",
    "with open(\"featurization_summary.json\", \"r\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "featurization_summary = pd.DataFrame.from_records(records)\n",
    "featurization_summary[\"Transformations\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = get_result_df(automl_run)\n",
    "best_dnn_run_id = summary_df[\"run_id\"].iloc[0]\n",
    "best_dnn_run = Run(experiment, best_dnn_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"AutoML_model\"  # Local folder where the model will be stored temporarily\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "best_dnn_run.download_file(\"outputs/model.pkl\", model_dir + \"/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "model_name = \"Titanic_AutoML_model\"\n",
    "best_model = Model.register(\n",
    "    model_path=model_dir + \"/model.pkl\", model_name=model_name, tags=None, workspace=ws\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Predict values for `Survived` for the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_predict = Dataset.Tabular.from_delimited_files(path='./data/test_modified.csv')\n",
    "dataset_predict_filtered = dataset_predict.keep_columns([\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Age\"])\n",
    "dataframe_predict_filtered = dataset_predict_filtered.to_pandas_dataframe()\n",
    "\n",
    "y_pred = best_model.predict(dataframe_predict_filtered)\n",
    "\n",
    "dataset_predict_output = dataset_predict.keep_columns([\"PassengerId\"])\n",
    "dataframe_predict_output = dataset_predict_output.to_pandas_dataframe()\n",
    "dataframe_predict_output = pd.concat([dataframe_predict_output, y_pred], axis=1)\n",
    "\n",
    "dataframe_predict_output.to_csv(\"./data/test_predict.csv\")\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "dataset_predict = Dataset.Tabular.register_pandas_dataframe(dataframe_predict_output,\n",
    "                                                            target=datastore,\n",
    "                                                            name=key+\"_filtered\",\n",
    "                                                            description=\"These are the preditions for the test dataset.\"\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "[Source1](https://docs.microsoft.com/de-de/python/api/overview/azure/ml/?view=azure-ml-py)\n",
    "[Source 2](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb)\n",
    "\n",
    "TODO: In the cell below, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "environment = Environment(\"Titanic-environment\")\n",
    "\n",
    "# Combine scoring script & environment in Inference configuration\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   environment=environment)\n",
    "\n",
    "# Set deployment configuration\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                       memory_gb = 1,\n",
    "                                                       enable_app_insights=True)\n",
    "\n",
    "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
    "service = Model.deploy(workspace = ws,\n",
    "                       name = \"Titanic-webservice\",\n",
    "                       models = [best_model],\n",
    "                       #inference_config = inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': dataset_predict_filtered[0:2].tolist(),\n",
    "    'method': 'predict'\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!python logs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the webservice\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
