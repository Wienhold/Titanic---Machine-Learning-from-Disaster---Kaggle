{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import helper\n",
    "import azureml.core\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workspace from config file present at .\\config.json.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Choose a name for experiment\n",
    "experiment_name = 'Titanic_hyperdrive'\n",
    "project_folder = './titanic-project-hyperdrive'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "found = False\n",
    "key = \"Titanic_dataset\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        datastore = ws.get_default_datastore()\n",
    "        datastore.upload(src_dir='data', target_path='data')\n",
    "        train_data = datastore.path('data/train_modified.csv')\n",
    "        \n",
    "        dataset = Dataset.Tabular.from_delimited_files(train_data, separator=';')        \n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=\"This is the complete dataset for the capstone project.\")\n",
    "        \n",
    "dataset_filtered = dataset.keep_columns([\"Survived\",\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Age\"])\n",
    "dataset_filtered = dataset_filtered.register(workspace=ws,\n",
    "                           name=key+\"_filtered\",\n",
    "                           description=\"This is the filtered dataset for the capstone project \" \\\n",
    "                           \"with only those features relevant for training.\")\n",
    "\n",
    "df = dataset_filtered.to_pandas_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "num_nodes = 5\n",
    "\n",
    "amlcompute_cluster_name = \"ComputeClusterCapstone\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           vm_priority = 'lowpriority',\n",
    "                                                           max_nodes=num_nodes)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "early_termination_policy = <your policy here>\n",
    "\n",
    "#TODO: Create the different params that you will be using during training\n",
    "param_sampling = <your params here>\n",
    "\n",
    "#TODO: Create your estimator and hyperdrive config\n",
    "estimator = <your estimator here>\n",
    "\n",
    "hyperdrive_run_config = <your config here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Submit your experiment\n",
    "remote_run = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run, best_model = remote_run.get_output()\n",
    "print(best_run)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "model_dir = \"Hyperdrive_model\"  # Local folder where the model will be stored temporarily\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "best_run.download_file(\"outputs/model.pkl\", model_dir + \"/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "# Register the model\n",
    "model_name = \"Titanic_Hyperdrive_model\"\n",
    "best_model = Model.register(workspace=ws,\n",
    "                            model_name=model_name,\n",
    "                            model_path=model_dir + \"/model.pkl\",\n",
    "                            model_framework=Model.Framework.SCIKITLEARN,\n",
    "                            model_framework_version=sklearn.__version__,\n",
    "                            sample_input_dataset=dataset_filtered,\n",
    "                            resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                            description=\"Hyperdrive model to predict Titanic survivors.\",\n",
    "                            tags=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "[Source1](https://docs.microsoft.com/de-de/python/api/overview/azure/ml/?view=azure-ml-py)\n",
    "[Source 2](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-to-cloud/model-register-and-deploy.ipynb)\n",
    "\n",
    "TODO: In the cell below, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment(Name: AzureML-AutoML,\n",
      "Version: 115)\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./score.py\n",
      "Uploaded ./score.py, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-06-03 09:25:39+00:00 Creating Container Registry if not exists.\n",
      "2022-06-03 09:25:39+00:00 Registering the environment.\n",
      "2022-06-03 09:25:40+00:00 Use the existing image.\n",
      "2022-06-03 09:25:40+00:00 Generating deployment configuration.\n",
      "2022-06-03 09:25:41+00:00 Submitting deployment to compute..\n",
      "2022-06-03 09:26:14+00:00 Checking the status of deployment titanic-webservice..\n",
      "2022-06-03 09:28:15+00:00 Checking the status of inference endpoint titanic-webservice.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "2022-06-03T09:27:30,803388000+00:00 - gunicorn/run \n",
      "2022-06-03T09:27:30,806797400+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,806912500+00:00 - iot-server/run \n",
      "2022-06-03T09:27:30,816039200+00:00 - nginx/run \n",
      "2022-06-03T09:27:30,820831000+00:00 | gunicorn/run | ###############################################\n",
      "2022-06-03T09:27:30,829672700+00:00 - rsyslog/run \n",
      "2022-06-03T09:27:30,826914300+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2022-06-03T09:27:30,864510100+00:00 | gunicorn/run | ###############################################\n",
      "2022-06-03T09:27:30,878130900+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,892807700+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,922439100+00:00 | gunicorn/run | AzureML image information: openmpi3.1.2-ubuntu18.04:20220516.v1\n",
      "2022-06-03T09:27:30,928814500+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,930806500+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,937325100+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml_76f657337a187f659ea5be5d48904769/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2022-06-03T09:27:30,940762500+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2022-06-03T09:27:30,943705900+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:30,948108800+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2022-06-03T09:27:31,230908500+00:00 - iot-server/finish 1 0\n",
      "2022-06-03T09:27:31,237352600+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "adal==1.2.7\n",
      "applicationinsights==0.11.10\n",
      "argcomplete==2.0.0\n",
      "arviz @ file:///tmp/build/80754af9/arviz_1614019183254/work\n",
      "attrs==21.4.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.22.1\n",
      "azure-graphrbac==0.61.1\n",
      "azure-identity==1.7.0\n",
      "azure-mgmt-authorization==2.0.0\n",
      "azure-mgmt-containerregistry==9.1.0\n",
      "azure-mgmt-core==1.3.0\n",
      "azure-mgmt-keyvault==9.3.0\n",
      "azure-mgmt-resource==21.0.0\n",
      "azure-mgmt-storage==20.0.0\n",
      "azure-storage-queue==12.3.0\n",
      "azureml-automl-core==1.42.0.post1\n",
      "azureml-automl-runtime==1.42.0.post1\n",
      "azureml-core==1.42.0\n",
      "azureml-dataprep==4.0.3\n",
      "azureml-dataprep-native==38.0.0\n",
      "azureml-dataprep-rslex==2.6.3\n",
      "azureml-dataset-runtime==1.42.0\n",
      "azureml-defaults==1.42.0\n",
      "azureml-inference-server-http==0.4.13\n",
      "azureml-interpret==1.42.0\n",
      "azureml-mlflow==1.42.0\n",
      "azureml-pipeline-core==1.42.0\n",
      "azureml-responsibleai==1.42.0\n",
      "azureml-telemetry==1.42.0\n",
      "azureml-train-automl-client==1.42.0.post1\n",
      "azureml-train-automl-runtime==1.42.0.post1\n",
      "azureml-train-core==1.42.0\n",
      "azureml-train-restclients-hyperdrive==1.42.0\n",
      "azureml-training-tabular==1.42.0.post1\n",
      "backcall==0.2.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==3.2.2\n",
      "bokeh==2.4.3\n",
      "boto==2.49.0\n",
      "boto3==1.15.18\n",
      "botocore==1.18.18\n",
      "cachetools==5.2.0\n",
      "certifi==2021.10.8\n",
      "cffi==1.15.0\n",
      "cftime @ file:///tmp/build/80754af9/cftime_1638345281172/work\n",
      "charset-normalizer==2.0.12\n",
      "click==7.1.2\n",
      "cloudpickle==1.6.0\n",
      "configparser==3.7.4\n",
      "contextlib2==21.6.0\n",
      "convertdate @ file:///tmp/build/80754af9/convertdate_1634070773133/work\n",
      "cryptography==36.0.2\n",
      "cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\n",
      "Cython @ file:///tmp/build/80754af9/cython_1647850341668/work\n",
      "dask==2.30.0\n",
      "databricks-cli==0.16.6\n",
      "dataclasses==0.6\n",
      "debugpy==1.6.0\n",
      "decorator==5.1.1\n",
      "dice-ml==0.7.2\n",
      "dill==0.3.5.1\n",
      "distributed==2.30.1\n",
      "distro==1.7.0\n",
      "docker==5.0.3\n",
      "dotnetcore2==3.1.23\n",
      "dowhy==0.7.1\n",
      "econml==0.12.0\n",
      "entrypoints==0.4\n",
      "ephem @ file:///tmp/build/80754af9/ephem_1638960312619/work\n",
      "erroranalysis==0.3.2\n",
      "fairlearn==0.7.0\n",
      "fbprophet @ file:///home/conda/feedstock_root/build_artifacts/fbprophet_1599365534439/work\n",
      "fire==0.4.0\n",
      "Flask==1.0.3\n",
      "flatbuffers==2.0\n",
      "fonttools==4.25.0\n",
      "fsspec==2022.5.0\n",
      "gensim==3.8.3\n",
      "gitdb==4.0.9\n",
      "GitPython==3.1.27\n",
      "google-api-core==2.8.1\n",
      "google-auth==2.6.6\n",
      "googleapis-common-protos==1.56.2\n",
      "gunicorn==20.1.0\n",
      "h5py==3.7.0\n",
      "HeapDict==1.0.1\n",
      "holidays @ file:///home/conda/feedstock_root/build_artifacts/holidays_1595448845196/work\n",
      "humanfriendly==10.0\n",
      "idna==3.3\n",
      "importlib-metadata==4.11.4\n",
      "importlib-resources==5.7.1\n",
      "inference-schema==1.3.2\n",
      "interpret-community==0.25.0\n",
      "interpret-core==0.2.7\n",
      "ipykernel==6.6.0\n",
      "ipython==7.34.0\n",
      "isodate==0.6.1\n",
      "itsdangerous==1.1.0\n",
      "jedi==0.18.1\n",
      "jeepney==0.8.0\n",
      "Jinja2==2.11.2\n",
      "jmespath==1.0.0\n",
      "joblib==0.14.1\n",
      "json-logging-py==0.2\n",
      "jsonpickle==2.2.0\n",
      "jsonschema==4.6.0\n",
      "jupyter-client==7.3.1\n",
      "jupyter-core==4.10.0\n",
      "keras2onnx==1.6.0\n",
      "kiwisolver @ file:///opt/conda/conda-bld/kiwisolver_1638569886207/work\n",
      "knack==0.9.0\n",
      "korean-lunar-calendar @ file:///tmp/build/80754af9/korean_lunar_calendar_1634063020401/work\n",
      "lightgbm==3.2.1\n",
      "llvmlite==0.36.0\n",
      "locket==1.0.0\n",
      "LunarCalendar @ file:///tmp/build/80754af9/lunarcalendar_1646383991234/work\n",
      "MarkupSafe==2.0.1\n",
      "matplotlib @ file:///tmp/build/80754af9/matplotlib-suite_1634667019719/work\n",
      "matplotlib-inline==0.1.3\n",
      "mkl-fft==1.3.0\n",
      "mkl-random==1.1.0\n",
      "mkl-service==2.3.0\n",
      "ml-wrappers==0.1.0\n",
      "mlflow-skinny==1.26.1\n",
      "mpi4py==3.1.3\n",
      "mpmath==1.2.1\n",
      "msal==1.18.0\n",
      "msal-extensions==1.0.0\n",
      "msgpack==1.0.3\n",
      "msrest==0.6.21\n",
      "msrestazure==0.6.4\n",
      "munkres==1.1.4\n",
      "ndg-httpsclient==0.5.1\n",
      "nest-asyncio==1.5.5\n",
      "netCDF4==1.5.7\n",
      "networkx==2.5\n",
      "nimbusml==1.8.0\n",
      "numba==0.53.1\n",
      "numpy==1.18.5\n",
      "oauthlib==3.2.0\n",
      "onnx==1.7.0\n",
      "onnxconverter-common==1.6.0\n",
      "onnxmltools==1.4.1\n",
      "onnxruntime==1.8.1\n",
      "opencensus==0.9.0\n",
      "opencensus-context==0.1.2\n",
      "opencensus-ext-azure==1.1.4\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1637314298585/work\n",
      "pandas==1.1.5\n",
      "paramiko==2.11.0\n",
      "parso==0.8.3\n",
      "partd==1.2.0\n",
      "pathspec==0.9.0\n",
      "patsy==0.5.2\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.0.1\n",
      "pkginfo==1.8.2\n",
      "pmdarima==1.7.1\n",
      "portalocker==2.4.0\n",
      "prompt-toolkit==3.0.29\n",
      "protobuf==3.20.1\n",
      "psutil @ file:///tmp/build/80754af9/psutil_1612298016854/work\n",
      "ptyprocess==0.7.0\n",
      "py-cpuinfo==5.0.0\n",
      "pyarrow==3.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "pydot==1.4.2\n",
      "Pygments==2.12.0\n",
      "PyJWT==2.4.0\n",
      "PyMeeus @ file:///tmp/build/80754af9/pymeeus_1634069098549/work\n",
      "PyNaCl==1.5.0\n",
      "pyOpenSSL==22.0.0\n",
      "pyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\n",
      "pyrsistent==0.18.1\n",
      "PySocks==1.7.1\n",
      "pystan @ file:///tmp/build/80754af9/pystan_1613565226242/work\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\n",
      "pytz==2021.3\n",
      "PyYAML==6.0\n",
      "pyzmq==23.1.0\n",
      "raiutils==0.0.1\n",
      "requests==2.27.1\n",
      "requests-oauthlib==1.3.1\n",
      "responsibleai==0.18.1\n",
      "rsa==4.8\n",
      "s3transfer==0.3.7\n",
      "scikit-learn==0.22.1\n",
      "scipy==1.5.2\n",
      "SecretStorage==3.3.2\n",
      "semver==2.13.0\n",
      "setuptools-git==1.2\n",
      "shap==0.39.0\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "skl2onnx==1.4.9\n",
      "sklearn-pandas==1.7.0\n",
      "slicer==0.0.7\n",
      "smart-open==1.9.0\n",
      "smmap==5.0.0\n",
      "sortedcontainers==2.4.0\n",
      "sparse==0.13.0\n",
      "statsmodels==0.11.1\n",
      "sympy==1.10.1\n",
      "tabulate==0.8.9\n",
      "tblib==1.7.0\n",
      "termcolor==1.1.0\n",
      "toolz==0.11.2\n",
      "tornado @ file:///tmp/build/80754af9/tornado_1606942283357/work\n",
      "tqdm @ file:///opt/conda/conda-bld/tqdm_1650891076910/work\n",
      "traitlets==5.2.2.post1\n",
      "typing-extensions @ file:///opt/conda/conda-bld/typing_extensions_1647553014482/work\n",
      "urllib3==1.26.9\n",
      "wcwidth==0.2.5\n",
      "websocket-client==1.3.2\n",
      "Werkzeug==1.0.1\n",
      "wrapt==1.12.1\n",
      "xarray @ file:///opt/conda/conda-bld/xarray_1639166117697/work\n",
      "xgboost==1.3.3\n",
      "zict==2.2.0\n",
      "zipp==3.8.0\n",
      "\n",
      "2022-06-03T09:27:32,726585100+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:32,733342600+00:00 | gunicorn/run | ###############################################\n",
      "2022-06-03T09:27:32,735388800+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2022-06-03T09:27:32,737610200+00:00 | gunicorn/run | ###############################################\n",
      "2022-06-03T09:27:32,743167300+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:32,744952600+00:00 | gunicorn/run | \n",
      "2022-06-03T09:27:32,746457100+00:00 | gunicorn/run | Starting HTTP server\n",
      "2022-06-03T09:27:32,754283100+00:00 | gunicorn/run | \n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://127.0.0.1:31311 (75)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 126\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2022-06-03 09:27:35,510 | root | INFO | Starting up app insights client\n",
      "DeprecationWarning: Explicitly using instrumentation key isdeprecated. Please use a connection string instead.\n",
      "DeprecationWarning: Explicitly using instrumentation key isdeprecated. Please use a connection string instead.\n",
      "DeprecationWarning: Explicitly using instrumentation key isdeprecated. Please use a connection string instead.\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2022-06-03 09:27:41,104 | root | INFO | Starting up request id generator\n",
      "2022-06-03 09:27:41,104 | root | INFO | Starting up app insight hooks\n",
      "2022-06-03 09:27:41,105 | root | INFO | Invoking user's init function\n",
      "2022-06-03 09:27:47,347 | azureml.core | WARNING | Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (msal-extensions 1.0.0 (/azureml-envs/azureml_76f657337a187f659ea5be5d48904769/lib/python3.7/site-packages), Requirement.parse('msal-extensions~=0.3.0'), {'azure-identity'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (msal-extensions 1.0.0 (/azureml-envs/azureml_76f657337a187f659ea5be5d48904769/lib/python3.7/site-packages), Requirement.parse('msal-extensions~=0.3.0'), {'azure-identity'}).\n",
      "/azureml-envs/azureml_76f657337a187f659ea5be5d48904769/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "00000000-0000-0000-0000-000000000000,Model loaded:\n",
      "00000000-0000-0000-0000-000000000000,Pipeline(memory=None,\n",
      "         steps=[('datatransformer',\n",
      "                 DataTransformer(enable_dnn=True, enable_feature_sweeping=True, feature_sweeping_config={}, feature_sweeping_timeout=86400, featurization_config=None, force_text_dnn=False, is_cross_validation=False, is_onnx_compatible=False, observer=None, task='classification', working_dir='/var/azureml-app')),\n",
      "                ('SparseNormalize...\n",
      "                 XGBoostClassifier(booster='gbtree', colsample_bytree=0.9, eta=0.2, gamma=0, max_depth=6, max_leaves=7, n_estimators=50, n_jobs=0, objective='reg:logistic', problem_info=ProblemInfo(gpu_training_param_dict={'processing_unit_type': 'gpu'}), random_state=0, reg_alpha=2.0833333333333335, reg_lambda=0.5208333333333334, subsample=0.6, tree_method='auto'))],\n",
      "         verbose=False)\n",
      "2022-06-03 09:27:49,270 | root | INFO | Users's init has completed successfully\n",
      "2022-06-03 09:27:49,279 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2022-06-03 09:27:49,280 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2022-06-03 09:27:49,281 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2022-06-03 09:28:15,498 | root | INFO | Swagger file not present\n",
      "2022-06-03 09:28:15,499 | root | INFO | 404\n",
      "127.0.0.1 - - [03/Jun/2022:09:28:15 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2022-06-03 09:28:16,754 | root | INFO | Swagger file not present\n",
      "2022-06-03 09:28:16,754 | root | INFO | 404\n",
      "127.0.0.1 - - [03/Jun/2022:09:28:16 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = Environment.get(ws, \"AzureML-AutoML\")\n",
    "print(env)\n",
    "for pip_package in [\"skikit-learn\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "# Update scoring script\n",
    "datastore.upload_files(['./score.py'], overwrite=True)\n",
    "\n",
    "# Combine scoring script & environment in Inference configuration\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\",\n",
    "                                   source_directory=\".\",\n",
    "                                   environment=env)\n",
    "\n",
    "# Set deployment configuration\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                       memory_gb = 1,\n",
    "                                                       enable_app_insights=True,\n",
    "                                                       auth_enabled=False)\n",
    "\n",
    "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
    "service = Model.deploy(workspace = ws,\n",
    "                       name = \"titanic-webservice\",\n",
    "                       models = [best_model],\n",
    "                       inference_config = inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       overwrite=True)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Predict values for `Survived` for the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AciWebservice(workspace=Workspace.create(name='quick-starts-ws-197423', subscription_id='81cefad3-d2c9-4f77-a466-99a7f541c7bb', resource_group='aml-quickstarts-197423'), name=titanic-webservice, image_id=None, image_digest=None, compute_type=ACI, state=Healthy, scoring_uri=http://8b4a0227-956e-4b86-8ac9-811bab24acc9.southcentralus.azurecontainer.io/score, tags=None, properties={'hasInferenceSchema': 'False', 'hasHttps': 'False'}, created_by={'userObjectId': '016836ad-a3f6-4af5-a9a5-b308069434a4', 'userPuId': '100320020175254D', 'userIdp': None, 'userAltSecId': None, 'userIss': 'https://sts.windows.net/660b3398-b80e-49d2-bc5b-ac1dc93b5254/', 'userTenantId': '660b3398-b80e-49d2-bc5b-ac1dc93b5254', 'userName': 'ODL_User 197423', 'upn': 'odl_user_197423@udacitylabs.onmicrosoft.com'})\n"
     ]
    }
   ],
   "source": [
    "service = Webservice(workspace=ws, name=\"titanic-webservice\")\n",
    "print(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def predict_from_df(df):\n",
    "    df_json = df.to_json(orient='records')\n",
    "\n",
    "    input_payload = json.dumps({\n",
    "        'data': json.loads(df_json),\n",
    "        'method': 'predict'\n",
    "    }, indent=2)\n",
    "    #print(input_payload)\n",
    "\n",
    "    output = service.run(input_payload)\n",
    "    #print(\"Response:\\n\",output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file\n",
    "found = False\n",
    "key = \"Titanic_dataset_test\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "    print(\"Found\", key)\n",
    "    found = True\n",
    "    dataset_predict = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "    print(\"Did not find\", key)\n",
    "    print(\"Create the dataset\")\n",
    "    # Create AML Dataset and register it into Workspace\n",
    "    datastore = ws.get_default_datastore()\n",
    "    datastore.upload(src_dir='data', target_path='data')\n",
    "    predict_data = datastore.path('data/test_modified.csv')\n",
    "\n",
    "    dataset_predict = Dataset.Tabular.from_delimited_files(predict_data, separator=';')        \n",
    "    dataset_predict = dataset_predict.register(workspace=ws,\n",
    "                               name=key,\n",
    "                               description=\"This is the test dataset for the capstone project.\")\n",
    "\n",
    "found = False\n",
    "key_filtered = key+\"_filtered\"\n",
    "\n",
    "if key_filtered in ws.datasets.keys(): \n",
    "    print(\"Found\", key_filtered)\n",
    "    found = True\n",
    "    dataset_predict_filtered = ws.datasets[key_filtered] \n",
    "\n",
    "if not found:\n",
    "    print(\"Did not find\", key)\n",
    "    print(\"Create the dataset\")\n",
    "    dataset_predict_filtered = dataset_predict.keep_columns([\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Age\"])\n",
    "    dataset_predict_filtered = dataset_predict_filtered.register(workspace=ws,\n",
    "                               name=key_filtered,\n",
    "                               description=\"This is the filtered test dataset for the capstone project \" \\\n",
    "                               \"with only those features relevant for prediction.\"\n",
    "                               )\n",
    "    \n",
    "print(dataset_predict_filtered[:2])\n",
    "dataframe_predict_filtered = dataset_predict_filtered.to_pandas_dataframe()\n",
    "\n",
    "y_pred = predict_from_df(dataframe_predict_filtered)\n",
    "\n",
    "dataset_predict_output = dataset_predict.keep_columns([\"PassengerId\"])\n",
    "dataframe_predict_output = dataset_predict_output.to_pandas_dataframe()\n",
    "dataframe_predict_output = pd.concat([dataframe_predict_output, pd.DataFrame(y_pred, columns=[\"Survived\"])], axis=1)\n",
    "\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "dataframe_predict_output.to_csv(\"dataset_test_predictions_hyperdrive.csv\", index=False)\n",
    "dataset_predict = Dataset.Tabular.register_pandas_dataframe(dataframe_predict_output,\n",
    "                                                            target=datastore,\n",
    "                                                            name=key+\"_filtered_predict\",\n",
    "                                                            description=\"These are the preditions for the test dataset.\"\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_from_df(dataframe_predict_filtered[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python logs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the webservice\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
